# Methods
<show-structure for="chapter,procedure" depth="2"/>

The deployment scenarios listed above touch upon the deployment methods that can be used in each scenario. In this
section we will go into more detail about what those methods are.

## Virtual Services

Virtual services are one of the core components of a load balancer. Where possible, you should configure the Load
Balancer to use Virtual Services to distribute traffic amongst the HyperStore nodes. Virtual Services are where all the
configuration resides for a particular service that is being load balanced by the appliance. There are, unsurprisingly,
a great many configuration options available within a Virtual Service but the options of principal concern with regards
to HyperStore Clusters are as follows:

### Virtual IP address (VIP)

A floating IP address that can be hosted on either the primary or secondary load balancer appliances.

### TCP Port(s)

The TCP port(s) that the virtual service will be listening on.

### Connection Distribution method

How the Virtual Service should distribute incoming requests on to the HyperStore Cluster. The default is for the
connections to be distributed to the Node with the least amount of active connections according to the load balancer.
The load balancer is tracking all the connections that are going through it and what the destination of those
connections is. Using this information the load balancer is able to determine which HyperStore node should be used.

### Session Persistence

The persistence tables keep track of what connections are going to which servers and ensure that subsequent connections
are forwarded to the same node . Whilst in a TCP conversation all traffic should be sent to the same HyperStore Node as
it is a stateful connection. Where persistence is a requirement, the persistence tables work in concert with the
connection distribution method/algorithm. The connection distribution method chooses which HyperStore Node a connection
should go to and the persistence table ensures that the subsequent packets from that TCP conversation also go to the
same node.

### Health checking

The load balancer performs scheduled health checks against the HyperStore nodes to verify that the nodes are online and
in an appropriate state to receive and process requests. The options for health checking are many and varied, for
HyperStore nodes they take the form of HTTP requests that are sent to predetermined endpoints. The response that it
received from the HTTP request to the HyperStore Node determines its availability to be included in the pool of servers
that the load balancer is distributing connections to.

## Global Server Load Balancing

GSLB is a technique that is used to distribute requests amongst Load Balancers, it is a way of load balancing load
balancers. Generally speaking these load balancers would be geographically dispersed or located in separate data centres
and/or different parts of the network. When we talk about GSLB with regards to HyperStore Clusters they are used to
provide access to a system via a single namespace. A request from a user would be sent to an FQDN such as
s3-usa.cloudian.com, the DNS infrastructure would forward the request to the GSLB(s) for resolution. The GSLB(s) would
respond with a record for a data centre that is available. The user’s request would then proceed to the designation DC
without knowing which one they are being served by. In the event that one of the DCs were to fail, the GSLB(s) would see
that it was failing health checks and it would be taken out of the pool. Requests for s3-usa.cloudian.com would not be
sent there until it was passing health checks again. The configuration for GSLB is limited in comparison with that of a
Virtual Service. This is because GSLB, typically, “sits on top of” a Virtual Service, extending it into a global
namespace.

### Global name

The global name is the FQDN that the GSLB will respond on the behalf of. Conceptually it is akin to the VIP of a Virtual
Service as it is the point of entry into the mechanisms of the GSLB.

### TTL

Time To Live is the amount of time in seconds that the DNS response that is generated by the GSLB should live in the
requester’s DNS cache. Be aware that TTL values are not always honoured by all systems. In the vast majority of use
cases DNS is a static record. The GSLB introduces a level of dynamism that DNS and the infrastructure that supports it
was not designed for. DNS servers and clients, quite reasonably, expect to be able to cache the results of DNS requests
for extended periods of time, this prevents unnecessary DNS resolutions against FQDNs that are largely unchanging. GSLB
with low TTL times flies in the face of this and you must therefore ensure that any intermediate DNS servers are not
being unhelpful when they cache responses from the GSLB…to be helpful.

#### TTL 0

From version 8.11 the option to configure Global Names with a TTL of 0 has been added to the GSLB. Configuring a TTL of
0 has been added to the product for a certain kind of workload although there are certainly a great number of scenarios
that would benefit from it. TTL0 provides the benefit of much more up-to-date information from the GSLB at the cost of
much more DNS traffic. When TTL0 is implemented there is absolutely no DNS caching and every request to the
namespace/FQDN results in a query to the GSLB for the correct record. Note that once the TCP connection is established
the source and destination IP addresses are set and do not change for the duration of the connection.

### Pools

A pool contains a group of members that requests should be distributed to for a particular global name. The
configuration of the pool contains the health check that should be performed against the member’s “monitor IP”, the
connection distribution method for the assigned members, and the members that are assigned to the pool. Pools are
assigned to Global names.

### Members

The specific endpoints that are to be added to the GSLB pool and can be returned to the requestor as an answer. For GSLB
configurations, the members are Virtual IP addresses (VIPs) corresponding to their respective Virtual Service.
Additionally you can configure a separate monitor IP, this is the IP addresses that health checks should be sent to for
this member. This is almost always the same as the IP address but it can be configured differently if required. Members
are assigned to Pools.

### Topologies

Topology configurations are only utilised when the Pool is set to use an “LB Method” of “twrr”. Topologies are used to
preferentially direct client requests from certain subnets to certain VIPs/GSLB members. This functionality is of
particular use when data centres are geographically dispersed and you want to configure the system so that the GSLB
directs users to the data centre that is closest to them. If that data centre is unavailable then it will instead direct
users to the alternate. Topology configurations should contain the CIDR subnets of both the users and the resource. For
example, if your VIP address is 10.10.10.10 and your users are coming from the subnet 10.50.50.0/24 you would need to
add “10.10.10.10/32, 10.50.50.0/24” to your topology configuration to tie the two address ranges together. Topologies
are assigned to Pools, but you must define a topology before you assign it to a pool.

### SDNS (Smart DNS)

Smart DNS is a specific implementation of the GSLB where the members of a pool are not the VIPs of Virtual Services but
they are the Front End addresses of the HyperStore Nodes themselves. SDNS does have all of the features of GSLB but it
does not have all of the features of Virtual Services. SDNS implementations should fall to the extremes of a HyperStore
Cluster installation. That is, it is configured on extremely small systems that are serviced by a virtualised load
balancer or it is configured on extremely large systems that are being hampered by the network throughput of the load
balancer. SDNS is alternatively named GSLB direct-to-node, and that is a good description of how it functions. Instead of
returning a record to the requester that is a VIP for a Virtual Service being provided by one of the load balancers, it
is directing the requester straight to one of the HyperStore nodes. This means that the traffic does not travel through
the load balancer but around the load balancer. SDNS is operating at the control plane but not at the data plane as that
part is entirely delegated to the network. SDNS is intelligently sending clients to the HyperStore Nodes based on the
results of health checks against those nodes.

- It works and is Reliable, Fast, Tunable, unlimited scalability and low-resource utilisation (i.e. cheaper).
- Works better and better with more servers/clients and more traffic, unlike layer 7, which has fixed limitations and
  higher resource requirements.
- It is only limited by the speed of the network infrastructure.
- Dynamic weighting, it is able to adjust the distribution of requests based on environmental factors
- TTL0 for bursty workloads. You can implement a Time To Live of 0 for the responses sent by SDNS, this provides
  requesters with the most up-to-date information and allows for better distribution of requests amongst the configured
  real servers.